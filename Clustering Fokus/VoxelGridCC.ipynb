{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb00c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "import os, json, time, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths (Windows)\n",
    "BASE = Path(r\"E:\\1.Clustering_TA\")\n",
    "RAW_DIR = BASE / \"dataset\"          # Afi, Kinan, Miftah (input mentah)\n",
    "OUT_DIR = BASE / \"clustered\"        # root output untuk hasil clustering\n",
    "EVAL_DIR = BASE / \"eval\"            # root output untuk metriks evaluasi\n",
    "FIG_DIR  = BASE / \"figs\"            # root output untuk gambar dokumentasi\n",
    "\n",
    "SUBJECTS = [\"Afi\", \"Kinan\", \"Miftah\"]   # folder subjek\n",
    "\n",
    "# --- Algo tag (akan jadi subfolder output)\n",
    "ALGO_NAME = \"vgcc_v08_minpt3_minvox8\"  # ubah otomatis saat HPO, jangan lupa update\n",
    "\n",
    "# --- Prefilter threshold (umum & ringan)\n",
    "SNR_MIN = 3.0             # buang pantulan lemah\n",
    "DOPPLER_ABS_MAX = 6.0     # m/s, buang doppler di luar nalar\n",
    "\n",
    "# --- VG parameters (akan di-sweep saat HPO)\n",
    "VOXEL_SIZE = 0.08                 # meter (≈ 8 cm)\n",
    "MIN_POINTS_PER_VOXEL = 3          # buang voxel terlalu jarang\n",
    "MIN_VOXELS_PER_CLUSTER = 8        # cluster minimal (6-10 cukup umum)\n",
    "\n",
    "# --- Visual doc: per file, simpan PNG untuk N frame contoh\n",
    "N_VIZ_FRAMES = 4\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# --- Kolom yang kita gunakan\n",
    "COLS = [\"timestamp\",\"frame\",\"x\",\"y\",\"z\",\"doppler\",\"SNR\"]\n",
    "\n",
    "# --- Utility: pastikan folder\n",
    "for d in [OUT_DIR, EVAL_DIR, FIG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f60245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_csv(path_csv: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path_csv)\n",
    "    # pastikan kolom ada\n",
    "    assert all(c in df.columns for c in COLS), f\"Kolom wajib tidak lengkap di {path_csv}\"\n",
    "    # coerce numeric\n",
    "    for c in [\"frame\",\"x\",\"y\",\"z\",\"doppler\",\"SNR\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    # timestamp: biarkan string apa adanya (sudah jadi kunci join)\n",
    "    return df.dropna(subset=[\"frame\",\"x\",\"y\",\"z\"]).copy()\n",
    "\n",
    "def prefilter_points(df_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    ok = (df_frame[\"SNR\"] >= SNR_MIN) & (df_frame[\"doppler\"].abs() <= DOPPLER_ABS_MAX)\n",
    "    return df_frame.loc[ok].copy()\n",
    "\n",
    "def voxelize_points(xyz: np.ndarray, voxel_size: float):\n",
    "    \"\"\"\n",
    "    xyz: (N,3) float\n",
    "    return:\n",
    "      vkeys: list of int3 tuple indices\n",
    "      vdict: dict voxel_key -> list of point indices\n",
    "      vcenters: dict voxel_key -> center (mean of member points)\n",
    "    \"\"\"\n",
    "    if xyz.size == 0:\n",
    "        return [], {}, {}\n",
    "    # integer voxel index\n",
    "    vids = np.floor(xyz / voxel_size).astype(np.int32)\n",
    "    vdict = {}\n",
    "    for idx, key in enumerate(map(tuple, vids)):\n",
    "        vdict.setdefault(key, []).append(idx)\n",
    "    # prune voxel by |points|\n",
    "    vdict = {k:v for k,v in vdict.items() if len(v) >= MIN_POINTS_PER_VOXEL}\n",
    "    vkeys = list(vdict.keys())\n",
    "    vcenters = {k: xyz[v].mean(axis=0) for k,v in vdict.items()}\n",
    "    return vkeys, vdict, vcenters\n",
    "\n",
    "# 6-neighborhood adjacency di grid (x±1,y±1,z±1) → gunakan 6 arah utama\n",
    "NEIGHBOR_DIRS = [(1,0,0),(-1,0,0),(0,1,0),(0,-1,0),(0,0,1),(0,0,-1)]\n",
    "\n",
    "def connected_components_voxels(vkeys: list):\n",
    "    \"\"\"\n",
    "    vkeys: list of 3D integer voxel keys\n",
    "    return: list of components (list of keys)\n",
    "    \"\"\"\n",
    "    if not vkeys:\n",
    "        return []\n",
    "    vset = set(vkeys)\n",
    "    visited = set()\n",
    "    comps = []\n",
    "    for k in vkeys:\n",
    "        if k in visited: \n",
    "            continue\n",
    "        # BFS\n",
    "        comp = []\n",
    "        stack = [k]\n",
    "        visited.add(k)\n",
    "        while stack:\n",
    "            cur = stack.pop()\n",
    "            comp.append(cur)\n",
    "            cx,cy,cz = cur\n",
    "            for dx,dy,dz in NEIGHBOR_DIRS:\n",
    "                nkey = (cx+dx, cy+dy, cz+dz)\n",
    "                if nkey in vset and nkey not in visited:\n",
    "                    visited.add(nkey)\n",
    "                    stack.append(nkey)\n",
    "        comps.append(comp)\n",
    "    return comps\n",
    "\n",
    "def choose_main_cluster(components, vdict):\n",
    "    \"\"\"\n",
    "    Pilih cluster utama = komponen dengan total member points terbanyak\n",
    "    \"\"\"\n",
    "    if not components:\n",
    "        return None, 0  # no cluster\n",
    "    sizes = []\n",
    "    for comp in components:\n",
    "        s = sum(len(vdict[k]) for k in comp)\n",
    "        sizes.append(s)\n",
    "    idx_max = int(np.argmax(sizes))\n",
    "    return components[idx_max], sizes[idx_max]\n",
    "\n",
    "def scatter3_save(points, labels, title, save_path: Path, elev=18, azim=30):\n",
    "    \"\"\"\n",
    "    points: (N,3) numpy\n",
    "    labels: (N,) int or None\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(6,5), dpi=140)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if points.size == 0:\n",
    "        ax.set_title(f\"{title}\\n(empty)\")\n",
    "    else:\n",
    "        if labels is None:\n",
    "            ax.scatter(points[:,0], points[:,1], points[:,2], s=8)\n",
    "        else:\n",
    "            # map labels to colors\n",
    "            labs = np.array(labels)\n",
    "            ulabs = np.unique(labs)\n",
    "            for li in ulabs:\n",
    "                m = (labs==li)\n",
    "                ax.scatter(points[m,0], points[m,1], points[m,2], s=8, label=f\"c{li}\")\n",
    "            ax.legend(loc=\"upper right\", fontsize=7)\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel(\"x [m]\"); ax.set_ylabel(\"y [m]\"); ax.set_zlabel(\"z [m]\")\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.grid(True)\n",
    "    fig.tight_layout()\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901b4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_frame(df_frame: pd.DataFrame, voxel_size: float):\n",
    "    \"\"\"\n",
    "    Input: satu frame (baris2 dengan 'frame' sama)\n",
    "    Output:\n",
    "      labels_per_point: np.array shape (N,) berisi cluster_id (0..C-1), -1 untuk noise/terbuang\n",
    "      metrics: dict frame-level (pc_count, num_clusters, kept_cluster_size, valid, latency_ms)\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    # sort by something stabil (opsional)\n",
    "    dfF = df_frame.sort_values(by=[\"x\",\"y\",\"z\"]).reset_index(drop=True)\n",
    "    pts = dfF[[\"x\",\"y\",\"z\"]].to_numpy(dtype=np.float32)\n",
    "    pc_count = len(dfF)\n",
    "\n",
    "    # prefilter\n",
    "    dfP = prefilter_points(dfF)\n",
    "    ptsP = dfP[[\"x\",\"y\",\"z\"]].to_numpy(dtype=np.float32)\n",
    "    if len(dfP)==0:\n",
    "        labels = np.full(pc_count, -1, dtype=np.int32)\n",
    "        latency_ms = (time.time()-t0)*1000\n",
    "        return labels, dict(pc_count=pc_count, num_clusters=0, kept_cluster_size=0, valid=0, latency_ms=latency_ms)\n",
    "\n",
    "    # voxelize (+ prune sparse voxels)\n",
    "    vkeys, vdict, _ = voxelize_points(ptsP, voxel_size)\n",
    "    if len(vdict)==0:\n",
    "        labels = np.full(pc_count, -1, dtype=np.int32)\n",
    "        latency_ms = (time.time()-t0)*1000\n",
    "        return labels, dict(pc_count=pc_count, num_clusters=0, kept_cluster_size=0, valid=0, latency_ms=latency_ms)\n",
    "\n",
    "    # connected components\n",
    "    comps = connected_components_voxels(vkeys)\n",
    "    # prune cluster kecil\n",
    "    comps = [c for c in comps if sum(len(vdict[k]) for k in c) >= MIN_VOXELS_PER_CLUSTER]\n",
    "\n",
    "    num_clusters = len(comps)\n",
    "    if num_clusters == 0:\n",
    "        labels = np.full(pc_count, -1, dtype=np.int32)\n",
    "        latency_ms = (time.time()-t0)*1000\n",
    "        return labels, dict(pc_count=pc_count, num_clusters=0, kept_cluster_size=0, valid=0, latency_ms=latency_ms)\n",
    "\n",
    "    # pilih komponen utama\n",
    "    main_comp, kept_size = choose_main_cluster(comps, vdict)\n",
    "\n",
    "    # berikan label ke semua points (di dfF) → mapping via dfP index\n",
    "    labels = np.full(pc_count, -1, dtype=np.int32)   # -1 = noise/terbuang\n",
    "    # label cluster id 0..num_clusters-1 sesuai urutan pada comps\n",
    "    comp_index = {id(comp):i for i,comp in enumerate(comps)}\n",
    "    # buat reverse map: point index in dfP -> cluster_id\n",
    "    p2lab = {}\n",
    "    for i,comp in enumerate(comps):\n",
    "        for vk in comp:\n",
    "            for pidx in vdict[vk]:\n",
    "                # pidx: index relatif ke dfP\n",
    "                p2lab[pidx] = i\n",
    "    # terapkan ke dfF melalui index join (cocokkan baris setelah prefilter)\n",
    "    # dfP dibuat dari subset dfF -> gunakan dfP.index sebagai pointer\n",
    "    for rel_pidx, lab in p2lab.items():\n",
    "        abs_idx = dfP.index[rel_pidx]  # index di dfF\n",
    "        labels[abs_idx] = lab\n",
    "\n",
    "    valid = 1 if kept_size > 0 else 0\n",
    "    latency_ms = (time.time()-t0)*1000\n",
    "\n",
    "    return labels, dict(\n",
    "        pc_count=pc_count,\n",
    "        num_clusters=num_clusters,\n",
    "        kept_cluster_size=kept_size,\n",
    "        valid=valid,\n",
    "        latency_ms=latency_ms\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4cba8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_session_metrics(per_frame_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Input: dataframe ringkas per frame:\n",
    "      ['frame','timestamp','pc_count','num_clusters','kept_cluster_size','valid','cx','cy','cz','dt','v_walk','latency_ms']\n",
    "    Output: dict metriks umum\n",
    "    \"\"\"\n",
    "    df = per_frame_df.copy()\n",
    "\n",
    "    # Valid Rate\n",
    "    VR = df[\"valid\"].mean() if len(df) else 0.0\n",
    "\n",
    "    # Largest Cluster Ratio (valid only)\n",
    "    msk = (df[\"valid\"]==1) & (df[\"pc_count\"]>0)\n",
    "    LCR_med = float((df.loc[msk, \"kept_cluster_size\"] / df.loc[msk, \"pc_count\"]).median()) if msk.any() else 0.0\n",
    "\n",
    "    # Median Cluster Size\n",
    "    MCS_med = float(df.loc[df[\"valid\"]==1, \"kept_cluster_size\"].median()) if (df[\"valid\"]==1).any() else 0.0\n",
    "\n",
    "    # Num-Clusters Median\n",
    "    NC_med = float(df[\"num_clusters\"].median()) if len(df) else 0.0\n",
    "\n",
    "    # v_walk stability (MAD)\n",
    "    v = df.loc[df[\"valid\"]==1, \"v_walk\"].dropna()\n",
    "    median_v = float(v.median()) if len(v) else 0.0\n",
    "    mad_v = float((v - median_v).abs().median()) if len(v) else 0.0\n",
    "\n",
    "    # Centroid Drop Rate: transisi valid/invalid\n",
    "    trans = 0\n",
    "    drops = 0\n",
    "    vals = df[\"valid\"].to_numpy(dtype=int)\n",
    "    for i in range(1, len(vals)):\n",
    "        if vals[i] != vals[i-1]:\n",
    "            trans += 1\n",
    "            drops += 1\n",
    "    CDR = drops / trans if trans > 0 else 0.0\n",
    "\n",
    "    # Latency median\n",
    "    LAT_med = float(df[\"latency_ms\"].median()) if \"latency_ms\" in df.columns and len(df) else 0.0\n",
    "\n",
    "    return dict(VR=VR, LCR_med=LCR_med, MCS_med=MCS_med, NC_med=NC_med, median_v=median_v, MAD_v=mad_v, CDR=CDR, LAT_med=LAT_med)\n",
    "\n",
    "def compute_centroid_and_vwalk(group_df: pd.DataFrame, labels: np.ndarray):\n",
    "    \"\"\"\n",
    "    hitung centroid (cluster utama = label 0) dan v_walk (butuh dt) → v_walk dihitung di loop sesi\n",
    "    \"\"\"\n",
    "    # ambil cluster utama (label==0)\n",
    "    m0 = (labels == 0)\n",
    "    if not m0.any():\n",
    "        return np.nan, np.nan, np.nan  # invalid\n",
    "    pts0 = group_df.loc[m0, [\"x\",\"y\",\"z\"]].to_numpy()\n",
    "    cx, cy, cz = pts0.mean(axis=0)\n",
    "    return float(cx), float(cy), float(cz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61347d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(subject: str, in_csv: Path, algo_name: str, save_figs=True):\n",
    "    \"\"\"\n",
    "    Proses 1 file:\n",
    "      - clustering per frame (VG-CC)\n",
    "      - tulis *_clustered_full.csv (semua titik + cluster_id)\n",
    "      - tulis eval metrics per frame & ringkasan sesi\n",
    "      - simpan beberapa figure sampel (warna per cluster_id)\n",
    "    \"\"\"\n",
    "    df = load_raw_csv(in_csv)\n",
    "    if df.empty:\n",
    "        print(f\"[WARN] kosong: {in_csv}\")\n",
    "        return None\n",
    "\n",
    "    # Output paths\n",
    "    out_dir  = OUT_DIR / algo_name / subject\n",
    "    eval_dir = EVAL_DIR / algo_name / subject\n",
    "    fig_dir  = FIG_DIR  / algo_name / subject\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # group-by frame\n",
    "    frames = sorted(df[\"frame\"].unique())\n",
    "    # siapkan tampungan\n",
    "    labels_all = np.full(len(df), -1, dtype=np.int32)\n",
    "    rows_metrics = []\n",
    "\n",
    "    # sampling frame untuk visualisasi\n",
    "    viz_frames = sorted(random.sample(frames, k=min(N_VIZ_FRAMES, len(frames))))\n",
    "\n",
    "    # loop frames\n",
    "    for fr in frames:\n",
    "        g = df.loc[df[\"frame\"]==fr]\n",
    "        labels, m = process_one_frame(g, VOXEL_SIZE)\n",
    "        # simpan label ke posisi baris global yang cocok\n",
    "        labels_all[g.index] = labels\n",
    "\n",
    "        # centroid + dt + v_walk\n",
    "        cx, cy, cz = compute_centroid_and_vwalk(g, labels)\n",
    "        ts = g[\"timestamp\"].iloc[0]\n",
    "        rows_metrics.append(dict(\n",
    "            frame=int(fr),\n",
    "            timestamp=str(ts),\n",
    "            pc_count=int(m[\"pc_count\"]),\n",
    "            num_clusters=int(m[\"num_clusters\"]),\n",
    "            kept_cluster_size=int(m[\"kept_cluster_size\"]),\n",
    "            valid=int(m[\"valid\"]),\n",
    "            cx=cx, cy=cy, cz=cz,\n",
    "            latency_ms=float(m[\"latency_ms\"])\n",
    "        ))\n",
    "\n",
    "        # simpan figure contoh\n",
    "        if save_figs and fr in viz_frames:\n",
    "            pts = g[[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "            ttl = f\"{subject} | {in_csv.stem} | frame {fr} | clusters={m['num_clusters']}\"\n",
    "            fpath = fig_dir / f\"{in_csv.stem}_frame{fr:05d}.png\"\n",
    "            scatter3_save(pts, labels, ttl, fpath)\n",
    "\n",
    "    # tulis clustered_full.csv (semua titik + cluster_id)\n",
    "    df_out = df.copy()\n",
    "    df_out[\"cluster_id\"] = labels_all\n",
    "    out_csv = out_dir / f\"{in_csv.stem}_clustered_full.csv\"\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "    # hitung dt & v_walk pada metrics frame-level\n",
    "    mdf = pd.DataFrame(rows_metrics).sort_values(\"frame\").reset_index(drop=True)\n",
    "    # dt dari timestamp: karena format string, kita gunakan delta antar frame berdasarkan urutan (fallback 1/15 s)\n",
    "    # Jika kamu punya timestamp real (detik float), konversi dulu.\n",
    "    # fallback Δt ~ 0.066 s (≈ 15 FPS)\n",
    "    DEFAULT_DT = 1/15.0\n",
    "    dts = []\n",
    "    vws = []\n",
    "    prev = None\n",
    "    for i, r in mdf.iterrows():\n",
    "        if prev is None:\n",
    "            dts.append(np.nan); vws.append(np.nan)\n",
    "        else:\n",
    "            # dt fallback\n",
    "            dt = DEFAULT_DT\n",
    "            # v_walk dari centroid xy\n",
    "            if np.isfinite(r[\"cx\"]) and np.isfinite(prev[\"cx\"]):\n",
    "                dx = r[\"cx\"] - prev[\"cx\"]\n",
    "                dy = r[\"cy\"] - prev[\"cy\"]\n",
    "                vw = math.sqrt(dx*dx + dy*dy) / dt\n",
    "            else:\n",
    "                vw = np.nan\n",
    "            dts.append(dt); vws.append(vw)\n",
    "        prev = r\n",
    "    mdf[\"dt\"] = dts\n",
    "    mdf[\"v_walk\"] = vws\n",
    "\n",
    "    # tulis per-frame metrics\n",
    "    per_frame_csv = eval_dir / f\"{in_csv.stem}_metrics.csv\"\n",
    "    mdf.to_csv(per_frame_csv, index=False)\n",
    "\n",
    "    # compute session metrics\n",
    "    sess = compute_session_metrics(mdf)\n",
    "    sess[\"file\"] = in_csv.name\n",
    "    sess[\"subject\"] = subject\n",
    "    sess[\"algo\"] = algo_name\n",
    "    sess[\"voxel_size\"] = VOXEL_SIZE\n",
    "    sess[\"min_pts_voxel\"] = MIN_POINTS_PER_VOXEL\n",
    "    sess[\"min_vox_cluster\"] = MIN_VOXELS_PER_CLUSTER\n",
    "    sess_json = eval_dir / f\"{in_csv.stem}_session_summary.json\"\n",
    "    with open(sess_json, \"w\") as f:\n",
    "        json.dump(sess, f, indent=2)\n",
    "\n",
    "    print(f\"[OK] {in_csv} → {out_csv.name} & metrics saved\")\n",
    "    return sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b64c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_raw_files(subject: str):\n",
    "    sdir = RAW_DIR / subject\n",
    "    return sorted([p for p in sdir.glob(\"*.csv\")])\n",
    "\n",
    "def run_batch_once(algo_name: str):\n",
    "    summaries = []\n",
    "    for subject in SUBJECTS:\n",
    "        files = list_raw_files(subject)\n",
    "        for p in files:\n",
    "            s = process_one_file(subject, p, algo_name, save_figs=True)\n",
    "            if s:\n",
    "                summaries.append(s)\n",
    "    if summaries:\n",
    "        df_sum = pd.DataFrame(summaries)\n",
    "        df_sum = df_sum[[\"subject\",\"file\",\"algo\",\"VR\",\"LCR_med\",\"MCS_med\",\"NC_med\",\"median_v\",\"MAD_v\",\"CDR\",\"LAT_med\",\n",
    "                         \"voxel_size\",\"min_pts_voxel\",\"min_vox_cluster\"]]\n",
    "        outf = EVAL_DIR / algo_name / f\"SUMMARY_{algo_name}.csv\"\n",
    "        outf.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_sum.to_csv(outf, index=False)\n",
    "        print(f\"[OK] SUMMARY saved at {outf}\")\n",
    "    else:\n",
    "        print(\"[WARN] No summaries generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a677dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid kecil & realistis, silakan sesuaikan\n",
    "VOXEL_CAND = [0.06, 0.08, 0.10]            # 6–10 cm\n",
    "MINPTS_VOX_CAND = [2, 3, 4]\n",
    "MINVOX_CLU_CAND = [6, 8, 10]\n",
    "\n",
    "def run_hpo_vgcc():\n",
    "    results = []\n",
    "    for vs in VOXEL_CAND:\n",
    "        for mpv in MINPTS_VOX_CAND:\n",
    "            for mvc in MINVOX_CLU_CAND:\n",
    "                # set param global (sengaja sederhana agar konsisten antar-cell)\n",
    "                global VOXEL_SIZE, MIN_POINTS_PER_VOXEL, MIN_VOXELS_PER_CLUSTER, ALGO_NAME\n",
    "                VOXEL_SIZE = vs\n",
    "                MIN_POINTS_PER_VOXEL = mpv\n",
    "                MIN_VOXELS_PER_CLUSTER = mvc\n",
    "                ALGO_NAME = f\"vgcc_v{int(vs*100):02d}_minpt{mpv}_minvox{mvc}\"\n",
    "\n",
    "                print(f\"\\n=== Running {ALGO_NAME} ===\")\n",
    "                run_batch_once(ALGO_NAME)\n",
    "\n",
    "                # kumpulkan summary global untuk ranking cepat\n",
    "                sum_path = EVAL_DIR / ALGO_NAME / f\"SUMMARY_{ALGO_NAME}.csv\"\n",
    "                if sum_path.exists():\n",
    "                    dfS = pd.read_csv(sum_path)\n",
    "                    # Skor sederhana: rata-rata VR tinggi, LCR_med tinggi, MAD_v rendah, LAT_med rendah\n",
    "                    dfS[\"score\"] = (dfS[\"VR\"]*0.5 + dfS[\"LCR_med\"]*0.4) - (dfS[\"MAD_v\"]*0.05 + dfS[\"LAT_med\"]*0.0005)\n",
    "                    avg = dfS[\"score\"].mean()\n",
    "                    results.append((ALGO_NAME, vs, mpv, mvc, float(avg)))\n",
    "    if results:\n",
    "        rank = sorted(results, key=lambda x: x[-1], reverse=True)\n",
    "        dfR = pd.DataFrame(rank, columns=[\"algo\",\"voxel_size\",\"min_pts_voxel\",\"min_vox_cluster\",\"score\"])\n",
    "        outf = EVAL_DIR / \"HPO_vgcc_results.csv\"\n",
    "        dfR.to_csv(outf, index=False)\n",
    "        print(f\"\\n[OK] HPO ranking saved → {outf}\")\n",
    "        print(dfR.head(10))\n",
    "    else:\n",
    "        print(\"[WARN] No HPO results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
